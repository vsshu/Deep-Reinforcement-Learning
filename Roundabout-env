import gym
import highway_env
from highway_env.envs import RoundaboutEnv
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.env_util import make_vec_env

from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy

import sys

import tensorflow as tf
#from stable_baselines.common import set_global_seeds

import pprint

def make_env(rank, seed=0):
    def _init():
        ##NORMAL 
        #col, speed, change, right, norm = -1, 0.2, -0.05, 0, True
        config = RoundaboutEnv.default_config()
        config['collision_reward'] = -5
        config["high_speed_reward"] = 0.001
        config["lane_change_reward"] = 0
        config["right_lane_reward"] = 0
        config["normalize_reward"] = False
        config["duration"] = 20
        env = RoundaboutEnv(config=config)
        
        env.seed(seed + rank)
        return env
    #set_global_seeds(seed)
    return _init

if __name__ == '__main__':
    num_cpu = 4
    env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])

    model = PPO('MlpPolicy', env, verbose=2).learn(3e4)
 
    # Evaluate the agent
    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)
    print(f"Mean reward = {mean_reward} +/- {std_reward}")

obs = model.env.reset()
for i in range(100):
    action, _states = model.predict(obs)
    obs, rewards, dones, info = model.env.step(action)
